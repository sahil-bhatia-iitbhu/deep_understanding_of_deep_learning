{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkHqutzZq2RHStjQxL6SnV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil-bhatia-iitbhu/deep_understanding_of_deep_learning/blob/main/17_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Style Transfer"
      ],
      "metadata": {
        "id": "stcjI-jK65_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature maps of the existing pretrained model are accessed, but not the its true mapping of features, but the related styling using concept of gram matrices."
      ],
      "metadata": {
        "id": "aA_6UAob69at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gram Matrices:\n",
        "- The concept is driven from the covariance matrix concept, where we get the pairwise interactions between the features.\n",
        "- Covariance matrix represents the patterns distributed across the featuresm not the indivisual features.\n",
        "- In style transfer, the feature covariance matrix encodes a higher-level feature-interaction space; a combination of many features.\n",
        "- Gram matrix is cross product of feature maps with itself to capture the patterns captured by all feature maps combined, and avoids deep training data specific patterns. Hence, only picking the style.\n",
        "\n",
        "**Note:** The 3D feature maps are converted into the 2D maps as below:\n",
        "\n",
        "Height x Width x Channels --> Channels x (Width*Height)"
      ],
      "metadata": {
        "id": "BeFZVEGf7ea6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Style Transfer Algorithm**\n",
        "\n",
        "1. Model is frozen, the image is trained. Prefer pretrained models trained on big datasets\n",
        "2. Import and transform images\n",
        "3. Match Content (\"pixel-level\" feature mapping): Prefer to use the early layers of the pretrained model to capture the style, and not the true image identities that are generally present in the built up layers.\n",
        "\n",
        "- Loss Function : $\\sum(Content Image - Target Image)^2$\n",
        "\n",
        "4. Match Style (\"texture-level\" feature mapping): Here, GRAM MATRICES from different frozen layers are used and compared to Gram Matrices of Target.\n",
        "\n",
        "- Loss Function : $\\sum(Style Gram - Target Gram)^2$"
      ],
      "metadata": {
        "id": "-U4hI_WzAZpZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mzV3kJt6aVwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fce4a95-0c21-48de-cf36-d096a0e0ff4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3204169295.py:19: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg')\n"
          ]
        }
      ],
      "source": [
        "### import libraries\n",
        "\n",
        "# for DL modeling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# to read an image from a url\n",
        "from imageio import imread\n",
        "\n",
        "# for number-crunching\n",
        "import numpy as np\n",
        "\n",
        "# for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the model\n",
        "vggnet = torchvision.models.vgg19(pretrained=True)\n",
        "\n",
        "# freeze all layers\n",
        "for p in vggnet.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# set to evaluation mode\n",
        "vggnet.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3vNCbapMZdi",
        "outputId": "db589ba2-e060-430d-d09f-88a52f5bc115"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548M/548M [00:02<00:00, 201MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# send the network to the GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "vggnet.to(device);"
      ],
      "metadata": {
        "id": "oT8Ful8wMh2R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img4content = imread('https://upload.wikimedia.org/wikipedia/commons/6/61/De_nieuwe_vleugel_van_het_Stedelijk_Museum_Amsterdam.jpg')\n",
        "img4style   = imread('https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg')\n",
        "\n",
        "# initialize the target image and random numbers\n",
        "img4target = np.random.randint(low=0,high=255,size=img4content.shape,dtype=np.uint8)\n",
        "\n",
        "print(img4content.shape)\n",
        "print(img4target.shape)\n",
        "print(img4style.shape)"
      ],
      "metadata": {
        "id": "mI_gFTpKMjiR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## These images are really large, which will make training take a long time.\n",
        "\n",
        "# create the transforms\n",
        "Ts = T.Compose([ T.ToTensor(),\n",
        "                 T.Resize(256),\n",
        "                 T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "               ])\n",
        "\n",
        "# apply them to the images (\"unsqueeze\" to make them a 4D tensor) and push to GPU\n",
        "img4content = Ts( img4content ).unsqueeze(0).to(device)\n",
        "img4style   = Ts( img4style   ).unsqueeze(0).to(device)\n",
        "img4target  = Ts( img4target  ).unsqueeze(0).to(device)\n",
        "\n",
        "print(img4content.shape)\n",
        "print(img4target.shape)\n",
        "print(img4style.shape)"
      ],
      "metadata": {
        "id": "sx7ZifsbMqh_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's have a look at the \"before\" pics\n",
        "fig,ax = plt.subplots(1,3,figsize=(18,6))\n",
        "\n",
        "pic = img4content.cpu().squeeze().numpy().transpose((1,2,0))\n",
        "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "ax[0].imshow(pic)\n",
        "ax[0].set_title('Content picture')\n",
        "\n",
        "pic = img4target.cpu().squeeze().numpy().transpose((1,2,0))\n",
        "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "ax[1].imshow(pic)\n",
        "ax[1].set_title('Target picture')\n",
        "\n",
        "pic = img4style.cpu().squeeze().numpy().transpose((1,2,0))\n",
        "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "ax[2].imshow(pic)\n",
        "ax[2].set_title('Style picture')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f3qGXGqmaOEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to extract image feature map activations"
      ],
      "metadata": {
        "id": "MqnhnxRpbX2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function that returns feature maps\n",
        "\n",
        "def getFeatureMapActs(img,net):\n",
        "\n",
        "  # initialize feature maps as a list\n",
        "  featuremaps = []\n",
        "  featurenames = []\n",
        "\n",
        "  convLayerIdx = 0\n",
        "\n",
        "  # loop through all layers in the \"features\" block\n",
        "  for layernum in range(len(net.features)):\n",
        "\n",
        "    # print out info from this layer\n",
        "    # print(layernum,net.features[layernum])\n",
        "\n",
        "    # process the image through this layer\n",
        "    img = net.features[layernum](img)\n",
        "\n",
        "    # store the image if it's a conv2d layer\n",
        "    if 'Conv2d' in str(net.features[layernum]):\n",
        "      featuremaps.append( img )\n",
        "      featurenames.append( 'ConvLayer_' + str(convLayerIdx) )\n",
        "      convLayerIdx += 1\n",
        "\n",
        "  return featuremaps,featurenames"
      ],
      "metadata": {
        "id": "OGLk9oFQbTeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function that returns the Gram matrix of the feature activation map\n",
        "\n",
        "def gram_matrix(M):\n",
        "\n",
        "  # reshape to 2D\n",
        "  _,chans,height,width = M.shape\n",
        "  M = M.reshape(chans,height*width)\n",
        "\n",
        "  # compute and return covariance matrix\n",
        "  gram = torch.mm(M,M.t()) / (chans*height*width)\n",
        "  return gram"
      ],
      "metadata": {
        "id": "SLjR0Fwxba-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the output of the function\n",
        "featmaps,featnames = getFeatureMapActs(img4content,vggnet)\n",
        "\n",
        "# print out some info\n",
        "for i in range(len(featnames)):\n",
        "  print('Feature map \"%s\" is size %s'%(featnames[i],(featmaps[i].shape)))"
      ],
      "metadata": {
        "id": "eXZ0V4W0bdOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# let's see what the \"content\" image looks like\n",
        "contentFeatureMaps,contentFeatureNames = getFeatureMapActs(img4content,vggnet)\n",
        "\n",
        "\n",
        "fig,axs = plt.subplots(2,5,figsize=(18,6))\n",
        "for i in range(5):\n",
        "\n",
        "  # average over all feature maps from this layer, and normalize\n",
        "  pic = np.mean( contentFeatureMaps[i].cpu().squeeze().numpy() ,axis=0)\n",
        "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "\n",
        "  axs[0,i].imshow(pic,cmap='gray')\n",
        "  axs[0,i].set_title('Content layer ' + str(contentFeatureNames[i]))\n",
        "\n",
        "\n",
        "  ### now show the gram matrix\n",
        "  pic = gram_matrix(contentFeatureMaps[i]).cpu().numpy()\n",
        "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "\n",
        "  axs[1,i].imshow(pic,cmap='gray',vmax=.1)\n",
        "  axs[1,i].set_title('Gram matrix, layer ' + str(contentFeatureNames[i]))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jpahataPbgtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat for the \"style\" image\n",
        "styleFeatureMaps,styleFeatureNames = getFeatureMapActs(img4style,vggnet)\n",
        "\n",
        "\n",
        "fig,axs = plt.subplots(2,5,figsize=(18,6))\n",
        "for i in range(5):\n",
        "\n",
        "  # average over all feature maps from this layer, and normalize\n",
        "  pic = np.mean( styleFeatureMaps[i].cpu().squeeze().numpy() ,axis=0)\n",
        "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "\n",
        "  axs[0,i].imshow(pic,cmap='hot')\n",
        "  axs[0,i].set_title('Style layer ' + str(styleFeatureNames[i]))\n",
        "\n",
        "\n",
        "  ### now show the gram matrix\n",
        "  pic = gram_matrix(styleFeatureMaps[i]).cpu().numpy()\n",
        "  pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "\n",
        "  axs[1,i].imshow(pic,cmap='hot',vmax=.1)\n",
        "  axs[1,i].set_title('Gram matrix, layer ' + str(styleFeatureNames[i]))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jCrwTY64bii_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now transfer the Style"
      ],
      "metadata": {
        "id": "1mNDalgeboSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# which layers to use\n",
        "layers4content = [ 'ConvLayer_1','ConvLayer_4' ]\n",
        "layers4style   = [ 'ConvLayer_1','ConvLayer_2','ConvLayer_3','ConvLayer_4','ConvLayer_5' ]\n",
        "weights4style  = [      1       ,     .5      ,     .5      ,     .2      ,     .1       ]"
      ],
      "metadata": {
        "id": "TB7xoIn1blY0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# make a copy of the target image and push to GPU\n",
        "target = img4target.clone()\n",
        "target.requires_grad = True\n",
        "target = target.to(device)\n",
        "styleScaling = 1e6\n",
        "\n",
        "# number of epochs to train\n",
        "numepochs = 1500\n",
        "\n",
        "# optimizer for backprop\n",
        "optimizer = torch.optim.RMSprop([target],lr=.005)\n",
        "\n",
        "\n",
        "for epochi in range(numepochs):\n",
        "\n",
        "  # extract the target feature maps\n",
        "  targetFeatureMaps,targetFeatureNames = getFeatureMapActs(target,vggnet)\n",
        "\n",
        "\n",
        "  # initialize the individual loss components\n",
        "  styleLoss = 0\n",
        "  contentLoss = 0\n",
        "\n",
        "  # loop over layers\n",
        "  for layeri in range(len(targetFeatureNames)):\n",
        "\n",
        "\n",
        "    # compute the content loss\n",
        "    if targetFeatureNames[layeri] in layers4content:\n",
        "      contentLoss += torch.mean( (targetFeatureMaps[layeri]-contentFeatureMaps[layeri])**2 )\n",
        "\n",
        "\n",
        "    # compute the style loss\n",
        "    if targetFeatureNames[layeri] in layers4style:\n",
        "\n",
        "      # Gram matrices\n",
        "      Gtarget = gram_matrix(targetFeatureMaps[layeri])\n",
        "      Gstyle  = gram_matrix(styleFeatureMaps[layeri])\n",
        "\n",
        "      # compute their loss (de-weighted with increasing depth)\n",
        "      styleLoss += torch.mean( (Gtarget-Gstyle)**2 ) * weights4style[layers4style.index(targetFeatureNames[layeri])]\n",
        "\n",
        "\n",
        "  # combined loss\n",
        "  combiloss = styleScaling*styleLoss + contentLoss\n",
        "\n",
        "  # finally ready for backprop!\n",
        "  optimizer.zero_grad()\n",
        "  combiloss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "bNCJFpc-bsbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# the \"after\" pic\n",
        "fig,ax = plt.subplots(1,3,figsize=(18,11))\n",
        "\n",
        "pic = img4content.cpu().squeeze().numpy().transpose((1,2,0))\n",
        "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "ax[0].imshow(pic)\n",
        "ax[0].set_title('Content picture',fontweight='bold')\n",
        "ax[0].set_xticks([])\n",
        "ax[0].set_yticks([])\n",
        "\n",
        "pic = torch.sigmoid(target).cpu().detach().squeeze().numpy().transpose((1,2,0))\n",
        "ax[1].imshow(pic)\n",
        "ax[1].set_title('Target picture',fontweight='bold')\n",
        "ax[1].set_xticks([])\n",
        "ax[1].set_yticks([])\n",
        "\n",
        "pic = img4style.cpu().squeeze().numpy().transpose((1,2,0))\n",
        "pic = (pic-np.min(pic)) / (np.max(pic)-np.min(pic))\n",
        "ax[2].imshow(pic,aspect=.6)\n",
        "ax[2].set_title('Style picture',fontweight='bold')\n",
        "ax[2].set_xticks([])\n",
        "ax[2].set_yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sbeCmksubusd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) The minimization loss has two components (style and content). Modify the code to store these two components in a\n",
        "#    Nx2 matrix (for N training epochs). Then plot them. This will help you understand and adjust the styleScaling gain\n",
        "#    factor.\n",
        "#\n",
        "# 2) Change the layers for minimizing losses to content and style images. Do you notice an effect of minimizing the\n",
        "#    earlier vs. later layers? How about more vs. fewer layers?\n",
        "#\n",
        "# 3) It's pretty neat to see the target image evolve over time. Modify the code to save the target image every, e.g.,\n",
        "#    100 epochs. Then you can make a series of images showing how the noise transforms into a lovely picture.\n",
        "#\n",
        "# 4) The target picture was initialized as random noise. But it doesn't need to be. It can be initialized to anything\n",
        "#    else. Try the following target initializations: (1) the content picture; (2) the style picture; (3) a completely\n",
        "#    different picture (e.g., a picture of you or a cat or the Taj Mahal).\n",
        "#"
      ],
      "metadata": {
        "id": "ms1SPmFDbxF5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}