{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2zBm1JQa1ZhJXm5ZjKGh4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil-bhatia-iitbhu/deep_understanding_of_deep_learning/blob/main/04_overfitting_and_cross_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Over Fitting and Cross Validation"
      ],
      "metadata": {
        "id": "hF2KrWfgLIIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting\n",
        "\n",
        "1. Overview\n",
        "2. Techniques to control overfitting\n",
        "3. Researcher Overfitting\n",
        "    \n",
        "    a. Also called researchers degree of freedom.\n",
        "\n",
        "    b. The researcher has many choices for how to clean, organize, and select the data; and which models and how many models to run.\n",
        "\n",
        "    Hence, as soon a data clean strategy and model architecture fits better than other iterations, the researcher prefers that model but that doesn't always mean that its the best model.\n",
        "\n",
        "4. Avoid Researcher Overfitting\n",
        "\n",
        "    a. Decide the architecture in advance, and make only minor adjustments. (Feasible for frequently-studied problems)\n",
        "\n",
        "    b. Build a series of models but never use the test set until all models are trained (kaggle-like competition and best of new problems or new models)"
      ],
      "metadata": {
        "id": "yZtDuWcyMcPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Validation\n",
        "\n",
        "1. Overview\n",
        "2. K-fold Cross validation\n",
        "3. Assumption of cross-validation:\n",
        "\n",
        "    The hold-out and test sets are independent of (uncorrelated with) the training set. If this assumption is violated, the relevance of this process is lost, and overfitting persists.\n",
        "\n",
        "4. Conclusion:\n",
        "    \n",
        "    Overfitting is not intrinsically bad, overfitting reduces generizability, which may or may not be problematic depending on the goals and scope of the model."
      ],
      "metadata": {
        "id": "ieMljIEqQKre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularization\n",
        "\n",
        "1. Generalization:\n",
        "    \n",
        "    How well the model works well, when applied on new data.\n",
        "\n",
        "2. Generalization Boundaries:\n",
        "\n",
        "    The population you want the model to apply to. This concept is really important as you can't apply every model on every scenario, you need to define the boundaries of your model itself.\n",
        "\n",
        "Hence, you could say that both over-fitting and regularization are relative to each other. The tighter the regularization boundaries, the more research overfitting you might need in regards to the data pertaining within boundaries.\n",
        "\n",
        "Though, training overfitting should ideally be in control."
      ],
      "metadata": {
        "id": "OZtDi2f1Zp7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-validation : Manually"
      ],
      "metadata": {
        "id": "qHXHC8dQmCBw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzV3kJt6aVwS"
      },
      "outputs": [],
      "source": []
    }
  ]
}